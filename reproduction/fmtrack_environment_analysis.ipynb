{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import meshio\n",
    "import subprocess\n",
    "import pickle\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def create_gp_model(X,Y,Z,QoI,alpha):\n",
    "    num_pts = X.shape[0]\n",
    "    X_train_unscale = np.zeros((num_pts,3))\n",
    "    X_train_unscale[:,0] = X\n",
    "    X_train_unscale[:,1] = Y\n",
    "    X_train_unscale[:,2] = Z\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train_unscale)\n",
    "    X_train = scaler.transform(X_train_unscale)\n",
    "    kernel = RationalQuadratic()\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=alpha)\n",
    "    gp.fit(X_train, QoI)\n",
    "    return gp , scaler\n",
    "\n",
    "\n",
    "def save_gp_model(gp_U,gp_V,gp_W,scaler,foldername):                            \n",
    "    # saves the Gaussian process models to the specified folder                 \n",
    "    pickle.dump(gp_U, open(os.path.join(foldername,'gp_U_cleaned.sav'),'wb'))   \n",
    "    pickle.dump(gp_V, open(os.path.join(foldername,'gp_V_cleaned.sav'),'wb'))   \n",
    "    pickle.dump(gp_W, open(os.path.join(foldername,'gp_W_cleaned.sav'),'wb'))   \n",
    "    pickle.dump(scaler,open(os.path.join(foldername,'scaler_cleaned.sav'),'wb'))\n",
    "\n",
    "\n",
    "def train_gpr(gp_dest_dir, mesh, nonspur_mask, alphas):\n",
    "    X = mesh.points[nonspur_mask,0]\n",
    "    Y = mesh.points[nonspur_mask,1]\n",
    "    Z = mesh.points[nonspur_mask,2]\n",
    "    U = mesh.point_data[\"u\"][nonspur_mask,0]\n",
    "    V = mesh.point_data[\"u\"][nonspur_mask,1]\n",
    "    W = mesh.point_data[\"u\"][nonspur_mask,2]\n",
    "\n",
    "    gp_U_cleaned, _ = create_gp_model(X,Y,Z,U,alphas[0])\n",
    "    gp_V_cleaned, _ = create_gp_model(X,Y,Z,V,alphas[1])\n",
    "    gp_W_cleaned,scaler_cleaned = create_gp_model(X,Y,Z,W,alphas[2])\n",
    "    \n",
    "    gp_stuff = (gp_U_cleaned,gp_V_cleaned,gp_W_cleaned,scaler_cleaned)\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(gp_dest_dir):\n",
    "            os.makedirs(gp_dest_dir)\n",
    "        save_gp_model(*gp_stuff, gp_dest_dir)\n",
    "    except Exception:\n",
    "        print(\"Encountered error attempting to save\")\n",
    "    \n",
    "    return gp_stuff\n",
    "\n",
    "\n",
    "_cache = dict()\n",
    "def get_predicted_u_hmvic(gpr_path, points):\n",
    "    # loads GPR models\n",
    "    if gpr_path not in _cache:\n",
    "        gp_U = pickle.load(open(os.path.join(gpr_path, 'gp_U_cleaned.sav'), 'rb'))\n",
    "        gp_V = pickle.load(open(os.path.join(gpr_path, 'gp_V_cleaned.sav'), 'rb'))\n",
    "        gp_W = pickle.load(open(os.path.join(gpr_path, 'gp_W_cleaned.sav'), 'rb'))\n",
    "        scaler = pickle.load(open(os.path.join(gpr_path, 'scaler_cleaned.sav'),'rb'))\n",
    "        _cache[gpr_path] = (gp_U, gp_V, gp_W, scaler)\n",
    "    else:\n",
    "        (gp_U, gp_V, gp_W, scaler) = _cache[gpr_path]\n",
    "\n",
    "    input_arr = scaler.transform(points)\n",
    "\n",
    "    N = 310000\n",
    "    if len(input_arr) < N:\n",
    "        u = gp_U.predict(input_arr)\n",
    "        v = gp_V.predict(input_arr)\n",
    "        w = gp_W.predict(input_arr)\n",
    "        disp = np.column_stack((u,v,w))\n",
    "    else:\n",
    "        # Must stage it\n",
    "        disp = np.zeros_like(input_arr)\n",
    "        for batch_i in range(int(np.ceil(len(input_arr)/N))):\n",
    "            u = gp_U.predict(input_arr[N*batch_i:N*(batch_i+1)])\n",
    "            v = gp_V.predict(input_arr[N*batch_i:N*(batch_i+1)])\n",
    "            w = gp_W.predict(input_arr[N*batch_i:N*(batch_i+1)])\n",
    "            disp[N*batch_i:N*(batch_i+1)] = np.column_stack((u,v,w))\n",
    "\n",
    "    return disp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is not meant to be run in the same Python environment as the inverse model. Please use an environment suitable for the object-oriented version of [FM-Track](https://github.com/elejeune11/FM-Track/tree/objectoriented).\n",
    "\n",
    "See the fenics_environment_analysis.ipynb notebook for guidance on obtaining data, and change directory to it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd YOUR_DATA_DIR_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPR Interpolation of Noisy Micro-Sphere Displacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use Gaussian Process Regression to interplate ground-truth displacements generated with a forward simulation with a finer mesh, and then synthetic noise was added, onto a coarser mesh. This emulates the case of real noisy micro-sphere data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_levels = \"noisy_target_test/noise_lvls.txt\"\n",
    "alphas = np.loadtxt(noise_levels)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = \"noisy_target_test\"\n",
    "cell_data_dir = \"cell_data_A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_case(alpha_stiff_amp):\n",
    "    bead_init = np.loadtxt(os.path.join(dest_dir, f\"bead_init_{alpha_stiff_amp}_nic.txt\"))\n",
    "    bead_u = np.loadtxt(os.path.join(dest_dir, f\"bead_u_{alpha_stiff_amp}_nic.txt\"))\n",
    "\n",
    "    pm_bead_mesh = meshio.Mesh(bead_init, [], point_data = {\"u\" : bead_u})\n",
    "\n",
    "    gp_dest_dir = os.path.join(dest_dir, f\"gp_denoise_{alpha_stiff_amp}_nic\")\n",
    "\n",
    "    gp_stuff = train_gpr(gp_dest_dir, pm_bead_mesh, np.ones(len(pm_bead_mesh.points)).astype(bool), alphas)\n",
    "    print(\"Finished GPR\")\n",
    "    \n",
    "    return gp_dest_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished GPR\n"
     ]
    }
   ],
   "source": [
    "alpha_stiff_amp = 1000\n",
    "\n",
    "gp_dest_dir = prepare_test_case(alpha_stiff_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='conda run -p /home/gpeery/miniconda3/envs/newestfenics get_u -c cell_data_A -g noisy_target_test/gp_denoise_1000_nic -o noisy_target_test/check/u_exp_1000.xdmf', returncode=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_points = np.loadtxt(os.path.join(cell_data_dir, \"cell_vertices_initial.txt\"))\n",
    "u = get_predicted_u_hmvic(gp_dest_dir, init_points)\n",
    "cell_mesh = meshio.Mesh(init_points, {}, point_data={\"u\":u})\n",
    "cell_mesh.write(os.path.join(dest_dir, \"bci.vtk\"))\n",
    "\n",
    "gel_mesh_boundaries = meshio.read(os.path.join(cell_data_dir, \"reference_domain_boundaries.xdmf\"))\n",
    "on_outer_boundary = np.zeros(len(gel_mesh_boundaries.points)).astype(bool)\n",
    "on_outer_boundary[gel_mesh_boundaries.cells[0].data[gel_mesh_boundaries.cell_data[\"boundaries\"][0] == 201]] = True\n",
    "bndry_points = gel_mesh_boundaries.points[on_outer_boundary]\n",
    "u = get_predicted_u_hmvic(gp_dest_dir, bndry_points)\n",
    "cube_mesh = meshio.Mesh(bndry_points, {}, point_data={\"u\":u})\n",
    "cube_mesh.write(os.path.join(dest_dir, \"bco.vtk\"))\n",
    "\n",
    "# get full shape u field\n",
    "conda_env = os.path.join(\n",
    "    os.path.dirname(os.environ[\"CONDA_PREFIX\"]),\n",
    "    \"newestfenics\"\n",
    ")\n",
    "output_file = os.path.join(dest_dir, f'u_exp_{alpha_stiff_amp}.xdmf')\n",
    "script_cmdline = f\"conda run -p {conda_env} get_u -c {cell_data_dir} -g {gp_dest_dir} -o {output_file}\"\n",
    "subprocess.run(script_cmdline, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
